server:
  port: 8082

spring:
  flyway:
    enabled: true
    baseline-on-migrate: true
    baseline-version: 0
    schemas: public
    locations: classpath:db/migration
  config:
    import: optional:secrets-dev.yml
  data:
    redis:
      url: localhost
      port: 6379
  servlet:
    multipart:
      max-file-size: 50MB
      max-request-size: 50MB

  cache:
    type: redis

  datasource:
    url: ${DATABASE_URL:jdbc:postgresql://localhost:5433/cognitia}
    username: ${DATABASE_USERNAME:cognitia}
    password: ${DATABASE_PASSWORD}
  ai:
    vectorstore:
      pgvector:
        index-type: HNSW
        distance-type: COSINE_DISTANCE
        dimensions: 1536
        max-document-batch-size: 10000
        initialize-schema: true
        # schema-name: coginita_vectorstore
        schema-validation: true
        datasource:
          url: ${VECTORSTORE_DATABASE_URL}
          username: ${VECTORSTORE_DATABASE_USERNAME:cognitia}
          password: ${VECTORSTORE_DATABASE_PASSWORD}
        # Move below config to secrets file
    openai:
      api-key: ${OPENAI_API_KEY}
      base-url: ${OPENAI_BASE_URL:https://generativelanguage.googleapis.com/v1beta/openai}
      embedding:
        options:
          model: ${OPENAI_EMBEDDING_MODEL:gemini-embedding-001}
          dimensions: ${OPENAI_EMBEDDING_DIMENSIONS:1536}
      chat:
        options:
          model: ${OPENAI_CHAT_MODEL:gemini-2.5-pro}
          stream-usage: true
        completions-path: ${OPENAI_COMPLETIONS_PATH:/chat/completions}

  
  # huggingface:
  #   api-key: ${HUGGINGFACE_API_KEY}
  #   embedding-model: ${HUGGINGFACE_EMBEDDING_MODEL:nomic-ai/nomic-embed-text-v1.5}

  jpa:
    hibernate:
      ddl-auto: none
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: true
        jdbc.batch_size: 20
        order_updates: true
        order_inserts: true
        default_schema: public
    generate-ddl: false
    show-sql: true

  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    producer:
      properties:
        enable:
          idempotence: ${KAFKA_IDEMPOTENCE:true}

ingestion:
  topic:
    name: ${INGESTION_TOPIC_NAME:resource-ingestion-preprocessing}
  group:
    name: ${INGESTION_GROUP_NAME:ingestion-group}
  partitions: ${INGESTION_PARTITIONS:1}
  replicas: ${INGESTION_REPLICAS:1}

analytics:
  usage-events:
    group:
      name: ${USAGE_EVENTS_GROUP_NAME:usage-events-group}
    topic:
      name: ${USAGE_EVENTS_TOPIC_NAME:chat-usage-events}  

# Kafka Configuration
# logging.level.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator=DEBUG
# logging.level.org.apache.kafka.clients.consumer=DEBUG
# logging.level.org.springframework.kafka.listener=DEBUG
# logging.level.org.springframework.kafka.consumer=DEBUG

# idempotency only works for retries from the same producer instance.

logging:
  level:
    '[com.intellidesk.cognitia.userandauth]': DEBUG
    org:
      apache:
        kafka: OFF
      aspectj:
        weaver: DEBUG
      springframework:
        aop: DEBUG
        tools: DEBUG
        ai:
          client:
            chat: DEBUG
          tool: DEBUG
          observation: DEBUG
          chat:
            memory: DEBUG
            client:
              advisor: DEBUG
              ChatMemoryAdvisor: DEBUG

storage:
  cloudinary_url: ${CLOUDINARY_URL}

jwt:
  public-key:
    path: classpath:keys/public.pem
  private-key:
    path: classpath:keys/private.pem

refresh:
  secret: ${REFRESH_TOKEN_SECRET}

cookie:
  secure: true  # Set to true when using Cloudflare tunnel (HTTPS) or production
  sameSite: None  # Required for cross-origin requests (frontend on different machine)


tavily:
  api:
    key: ${TAVILY_API_KEY}
    url: https://api.tavily.com/search
    max_results: 3
    topic: general
    search_depth: basic
    chunks_per_source: 3
    include_answer: advanced
    include_raw_content: true
    include_images: false
    include_favicon: false
